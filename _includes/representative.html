<ul>
<li><a href="https://arxiv.org/pdf/2402.12530">Parallel Structures in Pre-training Data Yield In-Context Learning</a>.<br>Yanda Chen, Chen Zhao, Zhou Yu, Kathleen McKeown and He He. <i>Association for Computational Linguistics (ACL)</i>, 2024.  [<a href="javascript:copy(div7, bib7)">bib</a>]
[<a href="https://github.com/yandachen/ParallelStructuresICL">code</a>]<br>
<div id="div7"></div><div id="bib7" style="display:none">
<div class="bib">
<pre>
@inproceedings{chen2024parallel,
        author={Yanda Chen and Chen Zhao and Zhou Yu and Kathleen McKeown and He He},
        title={Parallel Structures in Pre-training Data Yield In-Context Learning},
        booktitle={Association for Computational Linguistics (ACL)},
        year={2024}
}
</pre>
</div>
</div> </li>
<li><a href="https://arxiv.org/abs/2309.05196">Does Writing with Language Models Reduce Content Diversity?</a>.<br>Vishakh Padmakumar and He He. <i>International Conference on Learning Representations (ICLR)</i>, 2024.  [<a href="javascript:copy(div13, bib13)">bib</a>]
[<a href="https://github.com/vishakhpk/hai-diversity">code</a>]<br>
<div id="div13"></div><div id="bib13" style="display:none">
<div class="bib">
<pre>
@inproceedings{padmakumar2024writing,
        author={Vishakh Padmakumar and He He},
        title={Does Writing with Language Models Reduce Content Diversity?},
        booktitle={International Conference on Learning Representations (ICLR)},
        year={2024}
}
</pre>
</div>
</div> </li>
<li><a href="https://arxiv.org/abs/2210.01240">Language Models are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought</a>.<br>Abulhair Saparov and He He. <i>International Conference on Learning Representations (ICLR)</i>, 2023.  [<a href="javascript:copy(div26, bib26)">bib</a>]
[<a href="http://github.com/asaparov/prontoqa">code</a>]
[<a href="{% link /docs/presentation/prontoqa-slides.pdf %}">talk</a>]<br>
<div id="div26"></div><div id="bib26" style="display:none">
<div class="bib">
<pre>
@inproceedings{saparov2023language,
        author={Abulhair Saparov and He He},
        title={Language Models are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},
        booktitle={International Conference on Learning Representations (ICLR)},
        year={2023}
}
</pre>
</div>
</div> </li>
<li><a href="https://arxiv.org/pdf/2210.14011.pdf">Are All Spurious Features in Natural Language Alike? An Analysis through a Causal Lens</a>.<br>Nitish Joshi, Xiang Pan and He He. <i>Empirical Methods in Natural Language Processing (EMNLP)</i>, 2022.  [<a href="javascript:copy(div28, bib28)">bib</a>]
[<a href="https://github.com/joshinh/spurious-correlations-nlp">code</a>]
[<a href="{% link /docs/presentation/spurious-features-slides.pdf %}">talk</a>]<br>
<div id="div28"></div><div id="bib28" style="display:none">
<div class="bib">
<pre>
@inproceedings{joshi2022all,
        author={Nitish Joshi and Xiang Pan and He He},
        title={Are All Spurious Features in Natural Language Alike? An Analysis through a Causal Lens},
        booktitle={Empirical Methods in Natural Language Processing (EMNLP)},
        year={2022}
}
</pre>
</div>
</div> </li>
<li><a href="https://openreview.net/pdf?id=RovX-uQ1Hua">Text Generation by Learning from Demonstrations</a>.<br>Richard Yuanzhe Pang and He He. <i>International Conference on Learning Representations (ICLR)</i>, 2021.  [<a href="javascript:copy(div42, bib42)">bib</a>]
[<a href="https://github.com/yzpang/gold-off-policy-text-gen-iclr21">code</a>]
[<a href="{% link /docs/presentation/gold-slides.pdf %}">talk</a>]<br>
<div id="div42"></div><div id="bib42" style="display:none">
<div class="bib">
<pre>
@inproceedings{pang2021text,
        author={Richard Yuanzhe Pang and He He},
        title={Text Generation by Learning from Demonstrations},
        booktitle={International Conference on Learning Representations (ICLR)},
        year={2021}
}
</pre>
</div>
</div> </li>
<li><a href="https://arxiv.org/abs/2007.06778">An Empirical Study on Robustness to Spurious Correlations using Pre-trained Language Models</a>.<br>Lifu Tu, Garima Lalwani, Spandana Gella and He He. <i>Transaction of Association for Computational Linguistics (TACL)</i>, 2020.  [<a href="javascript:copy(div43, bib43)">bib</a>]
[<a href="https://github.com/lifu-tu/Study-NLP-Robustness">code</a>]<br>
<div id="div43"></div><div id="bib43" style="display:none">
<div class="bib">
<pre>
@article{tu2020empirical,
        author={Lifu Tu and Garima Lalwani and Spandana Gella and He He},
        title={An Empirical Study on Robustness to Spurious Correlations using Pre-trained Language Models},
        journal={TACL},
        volume={8},
        pages={},
        year={2020}
}
</pre>
</div>
</div> </li>
</ul>
